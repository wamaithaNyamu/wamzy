---
title: How to be a smart shylock
subTitle: Using classification algorithms to build a machine learning model that informs on whether to approve or reject loans.
date: 2021-10-24
author: Wamaitha Nyamu
authorLink: https://instagram.com/wamaithanyamu
categories: [ALGORITHMS]
featuredImage: ../assets/space-1.jpg
socialImage: ../assets/helium-social.jpg
featuredImageCaption: Photo by Wamaitha Nyamu
published: true

---
<SEO title="Loan prediction webapp" />

Angela has some disposable income that she wants to invest in a money lending business. This kind of business has several risk factors associated with it. How can Angela ake datadriven decisions on whom to lend money to?

<h2>Expectations</h2><br/>
The end goal is to make this a binary classification problem where given a set of inputs, the model either gives a prediction of approved or rejected. The approved prediction means Angela can go ahead and give the loan to the applicant while rejection means Angela will either have to revisit the decision manually or outright trust the model.

<h2>Loading the Data</h2>
For this problem, we will be using the dataset on loan prediction on Kaggle. You can download the data from <a href="https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset" target="_blank">here</a>.
<blockquote>You will need to use scikit-learn version 0.23.2 for the project to run smoothly.</blockquote>

```sh
 pip install -U scikit-learn==0.23.2

```
Import the relevant libraries needed

```py
    # Importing important libraries
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import plotly
    import plotly.express as px
    import plotly.figure_factory as ff
    from sklearn.utils import resample
    from sklearn.preprocessing import StandardScaler , MinMaxScaler
    from collections import Counter
    from scipy import stats

```

```py
    #Classifiers
    from sklearn.ensemble import AdaBoostClassifier , GradientBoostingClassifier , VotingClassifier , RandomForestClassifier
    from sklearn.linear_model import LogisticRegression , RidgeClassifier
    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
    from sklearn.model_selection import RepeatedStratifiedKFold
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.model_selection import GridSearchCV
    from sklearn.tree import DecisionTreeClassifier

    from sklearn import tree
    from sklearn.naive_bayes import GaussianNB
    from xgboost import plot_importance
    from xgboost import XGBClassifier
    from sklearn.svm import SVC
```
```py
    #Model evaluation tools
    from sklearn.metrics import classification_report , accuracy_score , confusion_matrix
    from sklearn.metrics import accuracy_score,f1_score
    from sklearn.model_selection import cross_val_score
```

```py
    #Data processing functions
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from sklearn import model_selection
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
```

Load the data from the csv. Replace <mark>"./path to csv"</mark> with where your data is located.

```py
train = pd.read_csv("./path to csv")
data = train.copy()

```
We load the dataset as a pandas dataframe and store it in <mark>train</mark>. We then make a copy of the train dataframe and store it in <mark>data</mark>. This allows to have a copy
of the original dataset then constantly manipulate the copy. If the need arises to use the original dataset, we will still have it to our disposal.

We can view our data as follows
```py
data.head(4)
```

<img src="../assets/shylock/dataframe.png" alt="dataframe image"/>

<h2>Exploratory data analysis</h2>
We first draw a scatter plot of all variables against themselves in a bivariate fashion.

```py
sns.pairplot(data)
plt.show()
```
<img src="../assets/shylock/pairplot.jpeg" alt="Pair plot image"/>
We need to understand the data we have to determine whether we have any outliers or missing values from the dataset. The describe function will help with this.

```py
data.describe()
```
<img src="../assets/shylock/describe.png" alt="Describe output"/>
From the above results from the describe function, we can see that some columns have missing values as indicated by the count. The standard deviation(std) also tells us more on how the data is distributed. Low standard deviation means data points are clustered around the mean, and high standard deviation indicates data is more spread out. We might need to investigate further on the data points on the ApplicantIncome and CoapplicantIncome columns as the std is too high which might indicate presence of outliers. Before we can do any manipulations we need to understand our column datatype using the info() function.

```py
data.info()

```
<img src="../assets/shylock/info.png" alt="Info output"/>
We have a dataset with 4 distinct datatype.

<h3>Handling outliers</h3>

From the std, we suspect having outliers in the Applicant Income and coapplicant income. Lets start by first visualising the distribution of the data in the ApplicantIncome to verify that we indeed have outliers that are accounting for the high std

```py
fig = px.scatter_matrix(data["ApplicantIncome"])
fig.update_layout(width=700,height=400)
fig.show()
```
<img src="../assets/shylock/applicant.png" alt="Applicant Income Outlier Graph"/>
The ApplicantIncome scatter plot visually confirms that the column has outliers. Next we work on the visualisation of the CoapplicantIncome column

```py
fig = px.scatter_matrix(data["CoapplicantIncome"])
fig.update_layout(width=700,height=400)
fig.show()
```
<img src="../assets/shylock/coapplicant.png" alt="Co-Applicant Income Outlier Graph"/>

As expected, the visual plot of the Coapplicantincome also shows our outliers.

<blockquote>To make work easier as the notebook grows, I will be writing all our model predictions on an external txt</blockquote>

```py
import os
if os.path.exists("results.txt"):
   os.remove("results.txt")
else:
   print("The file does not exist")
def write_results_to_file(txt):

  f = open("results.txt","a")
  f.write(txt + '\n')
  f.close()

def print_line(name):
    write_results_to_file(f'------------------------------------------------------------{name}----------------------------------')
    print(f'------------------------------------------------------------{name}----------------------------------')

```
The above code just removes any old files named results.txt on our file system and creates one if it does not exist. The write_results_to_file appends a new line to the results.txt while the
print_line adds a divider to the write_results_to_file file.

<h3>Checking the distribution of the data</h3>
Ideally,to get the best possible results our data needs to be in a normal distribution. Data in a normal distribution observes the bell curve.

```py
def check_distribution_of_column(column_name):
  fig = px.histogram(data[column_name],x =column_name,y = column_name)
  fig.update_layout(title=column_name)
  fig.show()


```
The function <mark>check_distribution_of_column</mark> takes in a column name and plots a histogram showing the distribution of data in that column.
We iterate through all the columns in our dataframe calling on our handy function to draw the distribution.

```py
for col in data.columns:
    print("Now showing the distribution of the " + col + " column:")
    check_distribution_of_column(col)
```
You should be having individual plots of each column. The loan amount plot looks like this
<img src="../assets/shylock/loandistribution.png" alt="Loan dsitribution plot"/>
From the above graphs we can see that we have a mix of categorical variables and discrete variables. Categorical variables contain a finite number of categories or distinct groups. Categorical data might not have a logical order. For example, categorical predictors for our dataset are gender,marital status,self employed etc. Discrete variables are numeric variables that have a countable number of values between any two values for example the applicants income, coapplicantincome and loan amount. With all these, we can now begin some data cleaning.

<h2>Data CLeaning </h2>
From our EDA we have established we have some problems we need to fix with the data :

1. Deal with null values to ensure our data is of same length
2. Deal with outliers
3. Find a way of converting categorical variables to discrete variables since our model will expect to use discrete values.
4. Find a way to make the data normally distributed

<h3>1. Dealing with null values</h3>
We have a few options:

1. Drop all rows with at least one null value
2. Replace the null value with either the mode, mean or median of the column.

For the first approach, we do not have much data to work with as such I will be exploring the second option. Also dropping the null values doesnt necessarily mean that we will not be dropping more rows as we continue with the data cleaning. We might need to drop more rows in future and thus greatly reduce our dataset. Option 2 is our best bet.

```py
# shows number of missing values by column
data.isnull().sum()
```
<img src="../assets/shylock/nullValues.png" alt="Null values"/>

For the categorical variables, I will use the mode of the column. For example, we have more men in the dataset so the probabilty of someone with a null value having being a man was higher than it being a woman.

```py
data["Gender"].fillna(data["Gender"].mode()[0],inplace=True)
data["Married"].fillna(data["Married"].mode()[0],inplace=True)
data["Self_Employed"].fillna(data["Self_Employed"].mode()[0],inplace=True)
data["Loan_Amount_Term"].fillna(data["Loan_Amount_Term"].mode()[0],inplace=True)
data["Dependents"].fillna(data["Dependents"].mode()[0],inplace=True)
data["Credit_History"].fillna(data["Credit_History"].mode()[0],inplace=True)

```
```py
# shows number of missing values by column
data.isnull().sum()
```
<img src="../assets/shylock/loanNull.png" alt="Null values"/>

Now checking for null values, we notice only the LoanAmount still has nulls which is a discrete variable.To fix this we will use the median since we had already established an outlier for this column. Thus using the mean might not give an accurate representation of roughly how much might have been borrowed.

```py
data["LoanAmount"].fillna(data["LoanAmount"].median(),inplace=True)

```


All done with dealing with null values.

<h3>2. Dealing with outliers</h3>
We have three discrete variables of which two have outliers.

```py
data["ApplicantIncome"] = np.log(data["ApplicantIncome"])
#As "CoapplicantIncome" columns has some "0" values we will get log values except "0" since log 0 is undefined
data["CoapplicantIncome"] = [np.log(i) if i!=0 else 0 for i in data["CoapplicantIncome"]]
data["LoanAmount"] = np.log(data["LoanAmount"])
```
We apply the natural logarithm to all our discrete variables to normalise the data. Natural log transformations reduce skewness to our datasets.
Compare our loan amount distribution now with what we had above

<img src="../assets/shylock/normLoan.png" alt="Null values"/>

<h3>3. Find a way of converting categorical variables to discrete variables since our model will expect to use discrete values.</h3>

With our normally distributed discrete data, we can now proceed to our categorical variables.

```py
discrete_columns=["ApplicantIncome","CoapplicantIncome","LoanAmount"]
Categorical_variables = np.array(list(filter(lambda x: x not in discrete_columns , data.columns)))

```

The above code stores all our categorical column names into a list.
The Loan_ID column does not help with our predictions, so we drop it. You will not qualify for a loan just because your loan id is a certain value.

```py
# Dropping the Loan_ID from the variables
Categorical_variables = np.delete(Categorical_variables,0)
```
Next we convert all our categorical variables to discrete variables

```py
def convert_categorical_to_discrete(col_name):
  data[col_name]=le.fit_transform(data[col_name])
```
We will loop through all categorical variable columns and transform the categorical variables to discrete. For example, on the married column, we have either a yes or no. Transforming this column converts all <mark>No</mark> entries to 0 and all <mark>Yes</mark> entries to 1.
```py
for col in Categorical_variables:
    convert_categorical_to_discrete(col)
```
Our dataframe now looks as follows
<img src="../assets/shylock/normdf.png" alt="Hot encoded df"/>

<h2>Model Building </h2>

We will use the following classification algorithms:

1. Logistic regression
2. SVM
3. Decision trees

Performance of the models will be measured using the recall, precision, f1score and accuracy perform for each model. A confusion matrix will be a great way to visualise the model performance.

Algorithms initialisations:

```py
# algorithm init
LOGC= LogisticRegression(solver='liblinear', random_state=0)
SVM=SVC(kernel="linear")
DTC=DecisionTreeClassifier()

```
For visualising the decision tree, the dtreeviz package will come in handy.

Install it using :

```shell
pip install dtreeviz
```
Then import it using :
```py

from dtreeviz.trees import dtreeviz # remember to load the package

```


Next we build a class that runs all our algorithms. The class code looks as follows:

```py
class ClassificationAlgorithms():
    def __init__(self, df,x,y):
      self.originalData= df
      self.data =  self.originalData.copy()
      self.X = x
      self.y = y
      self.X_train = 0
      self.X_test = 0
      self.y_train = 0
      self.y_test= 0


    def split_data(self):
       self.X = self.data.drop(["Loan_Status","Loan_ID"],axis=1)
       self.y = self.data["Loan_Status"]
       self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.3, random_state=42)

    def model_report(self,model,model_name):
        print_line(model_name)
        model.fit(self.X_train, self.y_train)
        y_predict = model.predict(self.X_test)
        print(classification_report(self.y_test, y_predict))
        write_results_to_file(classification_report(self.y_test, y_predict))
        sns.heatmap(confusion_matrix(self.y_test, y_predict), annot=True)
        plt.show()



    def logistic_regression(self):
        model = LOGC
        self.model_report(model,"LOGISTIC REGRESSION")

    def svm(self):
        model =SVM
        self.model_report(model,"SVM")



    def decision_trees(self):
        model = DTC
        self.model_report(model,"DECISION TREES")
        fig = plt.figure(figsize=(25,20))
        _ = tree.plot_tree(model,
                            feature_names=self.data.drop(["Loan_Status","Loan_ID"],axis=1).columns.to_list(),
                            class_names=['Y','N'],
                            filled=True)

```
Lets break down the class to understand what's going on.

Our __init__ function initialises all the global variables to be used in the class

```py
# initialise all our variables used in the class
  def __init__(self, df,x,y):
      # the class has three arguments, df - the data, x and y which is our feature and predictor values respectfully
      self.originalData= df
      # store a copy of our dataframe in data
      self.data =  self.originalData.copy()
      self.X = x
      self.y = y
      self.X_train = 0
      self.X_test = 0
      self.y_train = 0
      self.y_test= 0

```
The split_data first gets all the features we need for the models. We drop the Loan_Status column since its our target variable, and the Loan_ID column since it does not contribute to our predictions. Then we eventually split the data into training and test sets.

```py

  def split_data(self):
      # Features
       self.X = self.data.drop(["Loan_Status","Loan_ID"],axis=1)
     # Target variable
       self.y = self.data["Loan_Status"]
    # Split to test and training set
       self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.3, random_state=42)
```
The model_report function takes in a model and a model name as its parameters. The model is then fit to the training set and then used to make predictions on the test set. The predictions are then used to draw a confusion matrix. The output of the confusion matrix is then printed to the console and written in results.txt

```py

    def model_report(self,model,model_name):
        print_line(model_name)
        # fit model to
        model.fit(self.X_train, self.y_train)
        y_predict = model.predict(self.X_test)
        print(classification_report(self.y_test, y_predict))
        write_results_to_file(classification_report(self.y_test, y_predict))
        sns.heatmap(confusion_matrix(self.y_test, y_predict), annot=True)
        plt.show()


```

Lastly we have the three models used for the classification task.

```py
  def logistic_regression(self):
        model = LOGC
        self.model_report(model,"LOGISTIC REGRESSION")

    def svm(self):
        model =SVM
        self.model_report(model,"SVM")



    def decision_trees(self):
        model = DTC
        self.model_report(model,"DECISION TREES")
        fig = plt.figure(figsize=(25,20))
        _ = tree.plot_tree(model,
                            feature_names=self.data.drop(["Loan_Status","Loan_ID"],axis=1).columns.to_list(),
                            class_names=['Y','N'],
                            filled=True)

```
Now with the class explained lets build a function that utilises this class

```py

# test the algorithms

def _ClassificationAlgorithms(df,x,y):
  # initialise the class object
   c = ClassificationAlgorithms(df,x,y)
  # split the data
   c.split_data()
  # call the models
   c.logistic_regression()
   c.svm()
   c.decision_trees()
```

```py
# call the function and pass in our dataframe and initialise x and y to 0.
_ClassificationAlgorithms(data,0,0)

```

What we get :

1. For logistic regression
<img src="../assets/shylock/confusionMatrix.png" alt="Confusion Matrix"/>


2. For SVM
<img src="../assets/shylock/confusionMatrix.png" alt="Confusion Matrix"/>


3. For decision trees
<img src="../assets/shylock/confusionMatrix.png" alt="Confusion Matrix"/>
<img src="../assets/shylock/decision_tree.jpeg" alt="Decision tree graph"/>

All the confusion matrices show a common story of having very poor recall on the first class. This would suggest we might have an imbalanced dataset.


<h2>Can we do better?</h2>
<h4>Checking if our data is balanced</h4>

For our model to correctly predict the outcome, we need to have the two classes balanced in the datasets.

Lets visualise out data
```py
Approved = list(data.Loan_Status).count(1)
Rejected = list(data.Loan_Status).count(0)
print("Count of 1<Approved>: ",Approved,"\nCount of 0<Rejected>: ",Rejected)

fig = px.bar((Approved,Rejected),x=["Yes","No"],y=[Approved,Rejected],color=[Approved,Rejected])
fig.show()
```
<img src="../assets/shylock/dataCount.png" alt="Data balance"/>

The difference between these two classes is too big. As such the model does not have enough samples of the rejected classes to build a balanced model.

```py
event_rate = Rejected/len(data) * 100
event_rate
```
Our event rate means only 31.27035830618892% of the data is of the rejected class. Ideally we should get a 50-50 representation as some classification algorithm will tend to be biased toward the majority class. We shall use sampling techniques on the data. The main objective of balancing classes is to either increasing the frequency of the minority class (Rejected class) or decreasing the frequency of the majority class(Approved class).

<h3>Sampling techniques for the classification problem</h3>

Resampling the data helps curb the issue of data imbalance. Resampling options we have:

1. Random undersampling - randomly deleting entries from the majority class. The approved class for this case.

2. Random oversampling - randomly deleting entries from the minority class. The rejected class for our current dataset.

We will be using the imblearn package for sampling techniques.

To install it run :

```py
pip install imbalanced-learn
```

Import the relevant functions from imblearn with :

```py
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
from imblearn.pipeline import Pipeline
```

<blockquote>Please note for this package to work you need to have installed scikit-learn version 0.23.2</blockquote>

```py
pip install -U scikit-learn==0.23.2
```

Our resampling class :

```py


class ResamplingTechniques():

  def __init__(self,df):
     self.data = df.copy()
     self.X = 0
     self.y = 0
     self.X_train = 0
     self.X_test = 0
     self.y_train = 0
     self.y_test= 0


  def split_data(self):
       self.X = self.data.drop(["Loan_Status","Loan_ID"],axis=1)
       self.y = self.data["Loan_Status"]
       self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.3, random_state=42)

  def pipeline_imblearn(self,sample_type,sampler,model):
       # define pipeline
       steps = [(sample_type, sampler), ('model', model)]
       pipeline = Pipeline(steps=steps)
       # evaluate pipeline
       cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
       scores = cross_val_score(pipeline, self.X, self.y, scoring='f1_micro', cv=cv, n_jobs=-1)
       score = np.mean(scores)
       write_results_to_file('F1 Score: %.3f' % score)

       print('F1 Score: %.3f' % score)


  def random_sampling(self,name,sampler_model):

       print_line(name.upper() + " SAMPLING")
       print_line(f"LOGISTIC REGRESSION WITH {name.upper()} SAMPLING")
       self.pipeline_imblearn( name,sampler_model,LOGC)
       print_line(f"DECISION TREE WITH {name.upper()} SAMPLING")
       self.pipeline_imblearn( name,sampler_model,DTC)

       print_line(f"SVM WITH {name.upper()} SAMPLING")
       self.pipeline_imblearn( name, sampler_model ,SVM)

  def random_undersampling_approved(self):
      self.random_sampling('under',RandomUnderSampler(sampling_strategy='majority'))

  def random_oversampling_rejected(self):

      self.random_sampling('over', RandomOverSampler(sampling_strategy='minority'))


```

From the resampling class, the first two functions are similar to those in the Classification class thus I will not go over them.
```py
  def pipeline_imblearn(self,sample_type,sampler,model):
       # define pipeline
       steps = [(sample_type, sampler), ('model', model)]
       pipeline = Pipeline(steps=steps)
       # evaluate pipeline
       cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
       scores = cross_val_score(pipeline, self.X, self.y, scoring='f1_micro', cv=cv, n_jobs=-1)
       score = np.mean(scores)
       write_results_to_file('F1 Score: %.3f' % score)
       print('F1 Score: %.3f' % score)

```
 The pipeline_imblearn function takes in three arguments :

1. sample_type - this will either be over or under
2. sampler - either the oversampler or undersampler from imblearn
3. model - the model being run for that iteration. Between LGC, SVM and DTC

The pipeline return an fscore of the model

```py
  def random_sampling(self,name,sampler_model):

       print_line(name.upper() + " SAMPLING")
       print_line(f"LOGISTIC REGRESSION WITH {name.upper()} SAMPLING")
       self.pipeline_imblearn( name,sampler_model,LOGC)
       print_line(f"DECISION TREE WITH {name.upper()} SAMPLING")
       self.pipeline_imblearn( name,sampler_model,DTC)
       print_line(f"SVM WITH {name.upper()} SAMPLING")
       self.pipeline_imblearn( name, sampler_model ,SVM)

```

The random_sampling function takes in 2 arguments:

1. name - The name of the sampling technique being used either over or under. This is also passed on to the pipeline function.

2. sampler_model - either undersampler or oversampler from imblearn

```py

  def random_undersampling_approved(self):
      self.random_sampling('under',RandomUnderSampler(sampling_strategy='majority'))

  def random_oversampling_rejected(self):

      self.random_sampling('over', RandomOverSampler(sampling_strategy='minority'))

```

Lastly, we make two functions to call the samplers with the above two functions.

```py
def _ResamplingTechniques(data):
    z = ResamplingTechniques(data)
    z.split_data()
    z.random_oversampling_rejected()
    z.random_undersampling_approved()
```
The _ResamplingTechniques function initialises the ResamplingTechniques class and calls all the function in the class on our dataframe.

Run the function with :

```py
_ResamplingTechniques(data)
```

This yields the following fscores from our models


<img src="../assets/shylock/resamplingScores.png" alt="Resampling scores"/>

Performing the resampling has greatly improved our model. Can we do more? How about feature importance? Should gender hold more importance than marital status?

